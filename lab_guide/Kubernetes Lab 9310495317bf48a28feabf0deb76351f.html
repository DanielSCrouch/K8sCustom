<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Kubernetes Lab</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="d8783ca5-49ae-414d-b445-a316fd071544" class="page sans"><header><h1 class="page-title">Kubernetes Lab</h1></header><div class="page-body"><p id="7f201448-5c7c-43d8-8c7a-637e1f19558d" class="">
</p><h2 id="e448380c-2a36-4dea-b283-56579d4b27cc" class="">Cheatsheet</h2><pre id="da19782b-c2e4-449d-a7ec-f697ff837b1f" class="code"><code># View cluster configuration (kubeconfig files) 
$ kubectl config view
$ kubectl get pods 

# SSH into node instances
$ ssh dev@10.0.0.3

# Make a HTTP request for the Kubernetes version info:
curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version
# Expected output 
{
  &quot;major&quot;: &quot;1&quot;,
  &quot;minor&quot;: &quot;15&quot;,
  &quot;gitVersion&quot;: &quot;v1.15.3&quot;,
  &quot;gitCommit&quot;: &quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;,
  &quot;gitTreeState&quot;: &quot;clean&quot;,
  &quot;buildDate&quot;: &quot;2019-08-19T11:05:50Z&quot;,
  &quot;goVersion&quot;: &quot;go1.12.9&quot;,
  &quot;compiler&quot;: &quot;gc&quot;,
  &quot;platform&quot;: &quot;linux/amd64&quot;
}

# Component status 
kubectl get componentstatuses --kubeconfig admin.kubeconfig
</code></pre><h1 id="25432a4a-116f-46fb-a61d-d525ebd16e92" class="">Provisioning Compute Nodes </h1><p id="2fb1301b-0d8b-4312-bac3-1c84d5f62174" class="">Three nodes will be available to the cluster, a master node and two workers. Each is a Ubuntu Server 20.04 virtual machine instance, hosted with VirtualBox. A base image is initially created, and then adapted to meet the node performance and networking requirements (e.g. unique MAC addresses, IP and resource allocation). </p><ul id="873da03f-3f33-42db-913e-7fd4837055cf" class="bulleted-list"><li>Download Ubuntu Server 20.04 ISO from: <a href="https://ubuntu.com/download/server">https://ubuntu.com/download/server</a></li></ul><ul id="a03ebc8f-ccb8-4c43-b10a-0a9966b2fdcb" class="bulleted-list"><li>Download and install Virtual Box from: <a href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a></li></ul><ul id="15f10b25-b932-4006-8532-bc940b821416" class="bulleted-list"><li>From Virtual Box: <ol id="d3fc0a0c-be26-4840-9e74-6f6385af1317" class="numbered-list" start="1"><li>Create new image (2Gb RAM. 2 Cores, 10Gb storage) </li></ol><ol id="f12aa0b3-3632-4b23-8e7d-15059affe3c4" class="numbered-list" start="2"><li>Select &#x27;Create a virtual hard disk now&#x27; → VDI format → dynamic allocation </li></ol><ol id="a6790b27-3fc2-41a7-b04d-6eea651ede9c" class="numbered-list" start="3"><li>Create host-only network
File → Host Network Manager → Create 
name: vboxnet0,  dhcp disabled,  IP address: 10.0.0.1 (255.255.255.0) </li></ol><ol id="8cf7711f-d285-47eb-af8b-c2ccff7920c0" class="numbered-list" start="4"><li>Add network interfaces to image 
Select Ubuntu image → Settings → Network 
Adapter 1: Bridged Adapter (same interface as host, see ifconfig) 
Adapter 2: Host-only Adapter (vboxnet0) </li></ol></li></ul><ul id="0fda707b-00e7-4968-a133-74de024a9ad1" class="bulleted-list"><li>Ubuntu Server 20.04 installation <ol id="56300356-b766-4e63-b9d4-e4cfd4edbd41" class="numbered-list" start="1"><li>Start virtual machine, select Ubuntu ISO as optical disk file </li></ol><ol id="e1c944c7-e156-4d42-a1cb-5da0221e5533" class="numbered-list" start="2"><li>Name: dev   Server name: ubuntusvm   Username: dev   Password: ved </li></ol><ol id="dec80098-7fb9-4a82-9795-a87d161da370" class="numbered-list" start="3"><li>Select &#x27;Install OpenSSH server&#x27; </li></ol><ol id="337e08a0-2490-440d-b1c4-c53b43684e61" class="numbered-list" start="4"><li>Reboot, on request remove optical disk via &#x27;Devices&#x27; tab </li></ol></li></ul><ul id="f7c078ed-8f28-48b0-bea0-1c923b10f71d" class="bulleted-list"><li>Ubuntu image general configuration  <pre id="94c3b674-5d93-4f2e-98b2-e22430f10ba7" class="code"><code>$ sudo apt-get update 
$ sudo apt-get upgrade 
$ sudo apt-get install openssh-server   # if not already avaliable 
$ sudo apt install net-tools </code></pre></li></ul><ul id="08fcb8b9-1236-457a-8f17-f0281a535f54" class="bulleted-list"><li>Ubuntu image network configuration  <ol id="edc72c88-6b00-40ee-b2b9-fb68bc8c0ac1" class="numbered-list" start="1"><li>Record interface names (e.g. enp0s3, enp0s8 from ifconfig -a) </li></ol><ol id="fedd0d1d-ed93-470a-a2b7-8688a32dd147" class="numbered-list" start="2"><li>Modify (create if not present) /etc/netplan </li></ol><pre id="68622be2-76ea-4e4f-b257-a12daf6c58c6" class="code"><code>$ sudo nano /etc/netplan/01-netcfg.yaml 
$ sudo netplan apply </code></pre><pre id="63d3e23a-c991-42bc-ab55-9d31c85edb93" class="code"><code>network:
 version: 2
 renderer: networkd 
 ethernets:
  &lt;bridged interface name&gt;:
   dhcp4: yes 
   gateway4: &lt;gateway addr&gt; 
   nameservers:
    addresses: [&lt;gateway addr&gt;] 
  &lt;host-only interface name&gt; 
    dhcp4: no 
    addresses: [10.0.0.2/24]
    gateway4: 10.0.0.1</code></pre><p id="00b808e4-d35b-4068-bda6-8ca5fe1d4cea" class="">
</p></li></ul><p id="51d58895-2d68-4f03-988a-ef2dc0220a08" class="">
</p><ul id="c2df289f-77db-404d-90e7-1f44d0b76f0b" class="bulleted-list"><li>Differentiating compute nodes from base image. <ol id="3737fae6-8af4-44b4-b083-da2f6c972ff0" class="numbered-list" start="1"><li>Export Ubuntu image as .ova file (keep as backup) </li></ol><ol id="e0951f12-b4ee-4304-80c4-cf646fb5221a" class="numbered-list" start="2"><li>Proceed to re-import image for each instance required</li></ol><ol id="305e78ca-0679-450b-b0c7-149eab656115" class="numbered-list" start="3"><li>Import menu allows specification/edit of image name, and hardware resources</li></ol><div id="a9475a1f-e679-4bd5-9565-d57d0bae7137" class="collection-content"><h4 class="collection-title">Compute Nodes </h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesTitle"><path d="M7.73943662,8.6971831 C7.77640845,8.7834507 7.81338028,8.8943662 7.81338028,9.00528169 C7.81338028,9.49823944 7.40669014,9.89260563 6.91373239,9.89260563 C6.53169014,9.89260563 6.19894366,9.64612676 6.08802817,9.30105634 L5.75528169,8.33978873 L2.05809859,8.33978873 L1.72535211,9.30105634 C1.61443662,9.64612676 1.2693662,9.89260563 0.887323944,9.89260563 C0.394366197,9.89260563 0,9.49823944 0,9.00528169 C0,8.8943662 0.0246478873,8.7834507 0.0616197183,8.6971831 L2.46478873,2.48591549 C2.68661972,1.90669014 3.24119718,1.5 3.90669014,1.5 C4.55985915,1.5 5.12676056,1.90669014 5.34859155,2.48591549 L7.73943662,8.6971831 Z M2.60035211,6.82394366 L5.21302817,6.82394366 L3.90669014,3.10211268 L2.60035211,6.82394366 Z M11.3996479,3.70598592 C12.7552817,3.70598592 14,4.24823944 14,5.96126761 L14,9.07922535 C14,9.52288732 13.6549296,9.89260563 13.2112676,9.89260563 C12.8169014,9.89260563 12.471831,9.59683099 12.4225352,9.19014085 C12.028169,9.6584507 11.3257042,9.95422535 10.5492958,9.95422535 C9.60035211,9.95422535 8.47887324,9.31338028 8.47887324,7.98239437 C8.47887324,6.58978873 9.60035211,6.08450704 10.5492958,6.08450704 C11.3380282,6.08450704 12.040493,6.33098592 12.4348592,6.81161972 L12.4348592,5.98591549 C12.4348592,5.38204225 11.9172535,4.98767606 11.1285211,4.98767606 C10.6602113,4.98767606 10.2411972,5.11091549 9.80985915,5.38204225 C9.72359155,5.43133803 9.61267606,5.46830986 9.50176056,5.46830986 C9.18133803,5.46830986 8.91021127,5.1971831 8.91021127,4.86443662 C8.91021127,4.64260563 9.0334507,4.44542254 9.19366197,4.34683099 C9.87147887,3.90316901 10.6232394,3.70598592 11.3996479,3.70598592 Z M11.1778169,8.8943662 C11.6830986,8.8943662 12.1760563,8.72183099 12.4348592,8.37676056 L12.4348592,7.63732394 C12.1760563,7.29225352 11.6830986,7.11971831 11.1778169,7.11971831 C10.5616197,7.11971831 10.056338,7.45246479 10.056338,8.0193662 C10.056338,8.57394366 10.5616197,8.8943662 11.1778169,8.8943662 Z M0.65625,11.125 L13.34375,11.125 C13.7061869,11.125 14,11.4188131 14,11.78125 C14,12.1436869 13.7061869,12.4375 13.34375,12.4375 L0.65625,12.4375 C0.293813133,12.4375 4.43857149e-17,12.1436869 0,11.78125 C-4.43857149e-17,11.4188131 0.293813133,11.125 0.65625,11.125 Z"></path></svg></span>Node</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>Hostname</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>IP Address</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>RAM (Gb)</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>CPU Cores</th></tr></thead><tbody><tr id="7e266411-6104-4752-82fd-a0cfff4f0853"><td class="cell-title"><a href="https://www.notion.so/master-1-7e2664116104475282fda0cfff4f0853">master 1</a></td><td class="cell-OWyJ">kubemaster1</td><td class="cell-yWsQ">10.0.0.3</td><td class="cell-q&gt;)U">2</td><td class="cell-@|go">2</td></tr><tr id="d5cf5767-54c6-4598-ab71-4602ed1294fc"><td class="cell-title"><a href="https://www.notion.so/master-2-d5cf576754c64598ab714602ed1294fc">master 2</a></td><td class="cell-OWyJ">kubemaster2</td><td class="cell-yWsQ">10.0.0.4</td><td class="cell-q&gt;)U">2</td><td class="cell-@|go">2</td></tr><tr id="9eea5b33-397f-4dd9-9063-158e02206d6b"><td class="cell-title"><a href="https://www.notion.so/worker-1-9eea5b33397f4dd99063158e02206d6b">worker 1 </a></td><td class="cell-OWyJ">kubeworker1</td><td class="cell-yWsQ">10.0.0.5</td><td class="cell-q&gt;)U">1</td><td class="cell-@|go">1</td></tr><tr id="acc14c6f-ac0b-40d5-8dab-c976a46922b6"><td class="cell-title"><a href="https://www.notion.so/worker-2-acc14c6fac0b40d58dabc976a46922b6">worker 2</a></td><td class="cell-OWyJ">kubeworker2 </td><td class="cell-yWsQ">10.0.0.6</td><td class="cell-q&gt;)U">1</td><td class="cell-@|go">1</td></tr></tbody></table></div></li></ul><ul id="3667a629-c748-4455-b268-de817c8d7b3d" class="bulleted-list"><li>Change image <em>hostname</em> (from Ubuntu image terminal) <pre id="870eb72b-1029-476a-9011-6e1eb2eb87e8" class="code"><code>$ sudo hostnamectl set-hostname &lt;name&gt; 
$ sudoedit /etc/hosts      # update hostname</code></pre><p id="f55857ac-49d1-45b3-be85-a9ba0223a4fc" class="">
</p></li></ul><ul id="4fe8afee-77a3-4937-a862-7fb800597edb" class="bulleted-list"><li>Change image static IP address (host-only network) <pre id="c7419e17-d626-4bbb-8e28-5bd24e2db623" class="code"><code>$ sudo nano /etc/netplan/01-netcfg.yaml    # update &#x27;addresses&#x27; 
$ sudo netplan apply</code></pre></li></ul><ul id="898c1c0f-3322-4d15-97c1-174ea4542da5" class="bulleted-list"><li>Add SSH keys to each image for remote access. From host machine:<pre id="f4773227-6522-49fe-8ee1-1eb02f92ae32" class="code"><code>$ ls -al ~/.ssh      # check for existing keys 
$ keygen -o -b 4096 -t rsa     # generate keys, name them /id_rsa_k8s
# share key with node instance 
$ cat ~/.ssh/id_rsa_k8s.pub | ssh dev@10.0.0.3 &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot;
# repeat for each instance ip address (10.0.0.4 and 10.0.0.5) 
# may require creation of ~/.ssh directory on instances </code></pre></li></ul><ul id="45f5e6b6-30b0-44b8-9758-fe35514fba70" class="bulleted-list"><li>Images now be launched in header-less mode and accessed via ssh <pre id="2d41259b-6dd9-46a1-b19d-35432df8b534" class="code"><code>VBoxManage startvm &lt;image name&gt; --type headless
# wait until loaded 
$ ssh dev@10.0.0.3 
# to shutdown image from terminal 
VBoxManage controlvm &lt;image name&gt; poweroff
# or from within image terminal (via ssh) 
dev@kubecontroller:~/$ sudo shutdown -h now </code></pre></li></ul><h1 id="8784e409-ac69-4981-82a2-625d1e5d2b8d" class="">Kubernetes Architecture </h1><p id="a175bd2f-9b0b-4cfd-8417-cb06bc25e4bc" class="">A Kubernetes cluster consists of one or more machines called <strong>Nodes</strong> than run containerised applications. The worker node(s) host <strong>Pods. </strong> The Control Plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers, and a cluster usually runs multiple nodes, providing fault-tolerance and high availability. </p><p id="6ea5f796-2606-441b-97e3-88d309274c6e" class=""><strong>Control Plane:</strong> The container orchestration layer that exposes the API and interfaces </p><p id="034133d2-927d-4d09-b338-f595c0eefa04" class=""><strong>Pod:</strong> A set of running containers in the cluster </p><p id="3701f35e-8d11-4af3-b17a-33463d7d5060" class=""><strong>Node:</strong> Worker machine, e.g. physical or virtual compute resource </p><figure id="93104953-17bf-48a2-8fea-bf0deb76351f" class="image"><a href="Kubernetes%20Lab%209310495317bf48a28feabf0deb76351f/components.png"><img style="width:632px" src="Kubernetes%20Lab%209310495317bf48a28feabf0deb76351f/components.png"/></a></figure><p id="f88279d8-19ab-437d-b29b-141fd0736998" class=""><strong>kube-api-server</strong></p><p id="43a6959d-e121-40a4-93c3-2a3b43aff6fb" class="">The front end for the Kubernetes control plane, validates and configures data for the API objects which include pods, services, replication controllers, and others. </p><p id="8b537682-74b8-45ef-a0f9-83f6023f9c62" class=""><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</a></p><p id="892acdb2-18f6-43b5-a723-21c4678465ed" class=""><strong>kube-scheduler</strong> </p><p id="c6b8584a-1eba-4718-a632-ce1949df36aa" class="">A control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on. </p><p id="cc4596a1-2631-4327-99bb-4e0521883c52" class=""><strong>kube-controller-manager</strong></p><p id="b4342e08-496a-417c-a2aa-92fb8e0f2da0" class="">A control plane component that runs controller processes. A controller process is a loop that watches the shared state of the cluster through the kube-apiserver, and makes changes attempting to move the current state towards the declared desired state. </p><p id="2b48597f-e1a8-46fa-ae0a-2f5c87f6caac" class=""><strong>cloud-controller-manager </strong></p><p id="1978f4f3-af94-439c-9325-507decda7c31" class="">A control plane component that embeds cloud-specific control logic, enabling a link between the cluster and a cloud providers API. Hence, not required if outside of a cloud environment. </p><p id="d289ea6b-7b6b-49c4-aff3-0481e15930e9" class=""><strong>etcd </strong></p><p id="def5de6e-83cc-4d69-8cd7-33507501fbc8" class="">Consistent and highly-available key value store used as Kubernetes&#x27; &#x27;backing store&#x27; for all cluster data. </p><p id="8d6e2660-0846-464a-8355-32922884f01c" class=""><strong>kubelet </strong></p><p id="e8694ffd-f527-4bef-a077-080020d13e78" class="">An agent that runs on each node in the cluster. Ensures that containers are running within a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. </p><p id="81ff2870-493d-4cff-baa3-354b6b765bd6" class=""><strong>kube-proxy</strong> </p><p id="07f76c7e-6785-465e-a642-70da439767c1" class="">kube-proxy is a network proxy that runs on each node in the cluster, implementing part of the Kubernetes Service concept.
Maintains network rules on nodes. These network rules allow network communication to Pods from network sessions inside or outside of the cluster. kube-proxy uses the operating system packet filtering layer if there is one and it&#x27;s available. Otherwise, kube-proxy forwards the traffic itself. </p><p id="b42fbfa9-0346-4f5d-89bc-60ce5778bde6" class=""><strong>Container runtime</strong></p><p id="84172c97-4a58-4d17-a339-4fad4c7ad03f" class="">Software running on a worker node that is responsible for running containers. Several container runtimes are supported including Docker and containerd. </p><h1 id="d0fadad7-1ed5-45aa-ba73-26bd2409e1ec" class="">Cluster Authentication </h1><p id="997729b9-ff2e-45dd-9390-d55d1aa78e52" class="">Each component within the Kubernetes cluster must be able to verify and communicate with the API server. Authentication is managed through the Transfer Layer Security (TLS) protocol. </p><p id="b7570c92-6c23-4c07-a36c-1425915d8e5c" class="">Public Key Infrastructure (PKI) is a back-end cybersecurity measure for verifying user identities through digital certificates.</p><p id="76db957d-81a3-4a93-bda5-d07190a210c3" class=""><a href="https://blog.container-solutions.com/kubernetes-the-hard-way-explained-chapter-4">https://blog.container-solutions.com/kubernetes-the-hard-way-explained-chapter-4</a></p><p id="6dd57b39-4360-4f2b-9df4-ab77ddf6d69f" class="">Each component therefore, requires a certificate and a private key derived from a Certificate Authority (CA). </p><ul id="5ac0f7f1-0f89-40e7-87f8-efe9badeecd4" class="bulleted-list"><li>admin user (kubectl client)</li></ul><ul id="66511072-c739-4f1b-8cf3-605bc40af987" class="bulleted-list"><li>kubelet (one per worker node) </li></ul><ul id="939d7840-e48f-4d5c-bcad-9224964968af" class="bulleted-list"><li>kube-proxy </li></ul><ul id="feb11251-c64a-47cb-abce-cab4caf696e3" class="bulleted-list"><li>kube-controller-manager </li></ul><ul id="528ff447-7073-4e51-b84d-c637e971f2bf" class="bulleted-list"><li>kube-scheduler </li></ul><ul id="461a6fbf-b753-4ad5-9ea3-957ff66ebf48" class="bulleted-list"><li>kube-api-server </li></ul><ul id="6636aaad-51bc-40cb-9bb6-ded52306e7a2" class="bulleted-list"><li>service account </li></ul><p id="1ebc4460-d1a6-4429-962b-469e61a89ff7" class=""><em>Generate the keys on the host. There is no requirement for the worker nodes to have cfssl, and the certs and keys will be distributed later. </em></p><p id="e8b3900e-5d8d-431d-8593-7675607fc628" class=""><strong>Users</strong></p><p id="eb2fb6b1-da16-4cdf-bcfd-e7c03670e050" class="">All Kubernetes clusters have two categories of users: service accounts and normal users. API requests are tied to either type of user, or treated as anonymous requests. This means every process, inside or outside the cluster, must authenticate when making requests to the API server. </p><p id="fd5adff8-2afe-4aee-a4c3-aa42e90be49a" class=""><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">https://kubernetes.io/docs/reference/access-authn-authz/authentication/</a></p><p id="1cfc3a5b-f55e-427b-88d8-c8e56bec445f" class=""><strong>Normal users</strong></p><p id="1a5fb4a8-82bc-42a3-b423-76fdf7ab99d5" class="">Normal users are assumed to be managed by an outside service. Users interact with the cluster through the web API or the kubectl tool. Any user that presents a valid certificate signed by the cluster&#x27;s certificate authority (CA) is considered authenticated. Kubernetes determines the username from the common name field in the &#x27;subject&#x27; of the cert (e.g. &quot;/CN=bob&quot;). From there, the Role-Based Access Control (RBAC) sub-system would determine whether the user is authorized to perform a specific operation on a resource.</p><p id="88ec049c-58f9-4081-8d4d-76a0b883f9f7" class=""><a href="https://sysdig.com/blog/kubernetes-security-rbac-tls/">https://sysdig.com/blog/kubernetes-security-rbac-tls/</a></p><p id="52c22e79-c209-4f20-b92b-b02978c8c36b" class=""><a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/</a></p><p id="29efb23f-f4f5-4d76-9bf7-459351344956" class=""><a href="https://unofficial-kubernetes.readthedocs.io/en/latest/admin/accessing-the-api/">https://unofficial-kubernetes.readthedocs.io/en/latest/admin/accessing-the-api/</a></p><p id="23311322-bbc2-4416-bfa5-af3f084dcdce" class=""><strong>Service account</strong></p><p id="00d7d406-b71b-4473-be8f-440736831492" class="">Service accounts are users managed by the API. They are bound to specific namespaces, and created automatically by the API server or manually through API calls. Service accounts are tied to a set of credentials stored as <em>secrets</em>, which are mounted into pods allowing in-cluster processes to talk to the API. </p><p id="f9f70743-2e98-4024-b7c5-222a5f9a3ded" class=""><strong>Tools</strong></p><p id="47bcce06-1857-43c2-aabc-d4dfdd2f7aeb" class="">cfssl: enables signing, verifying, and bundling of TLS certificates</p><p id="bc9e5aff-d664-4454-828f-1268d5ebb7a5" class="">cfssljson: used to parse JSON responses from the cfssl server</p><p id="82cff638-0328-43b3-b777-dfe84f0d8584" class=""><a href="https://github.com/cloudflare/cfssl">https://github.com/cloudflare/cfssl</a></p><pre id="bbf23d88-fbfa-4332-8808-a004278e554b" class="code"><code>$ go get -u github.com/cloudflare/cfssl/cmd/cfssl
$ go get -u github.com/cloudflare/cfssl/cmd/cfssljson</code></pre><p id="2acb7bbd-3693-415f-9000-c758b51d2786" class="">
</p><p id="831b2cfe-862d-4524-a7bf-821c71c408a5" class=""><strong>Certification Authority (CA)</strong> </p><p id="41b62edb-17ee-401e-acc8-169ff78963c5" class="">A cluster is bootstrapped/provisioned as a Certification Authority (CA), providing a CA certificate and private key. </p><p id="83f7164a-aa80-4194-9129-74853527cafc" class="">Calls to the API must present a certificate that self-identifies the requester. This must be signed by the CA. Any API server, node, etc. that is going to sign a certificate therefore needs to be issued with a copy of the CA private key. </p><p id="4c0b2d02-50d5-48c7-9feb-fb8391294910" class=""><strong>ca-config.json: </strong>specifies the configuration of the certificate authority (expiration time and profile). </p><pre id="38fd9ac7-9505-4d08-b289-83027c5ed408" class="code"><code>cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
EOF</code></pre><p id="f37fb7bf-0e03-419a-b1d8-48558f9bef34" class=""><strong>ca-csr.json:</strong> is the certificate signing request that includes the information to validate new requests for certificates. </p><pre id="2427544d-53eb-4c8e-99a3-6e6bf776b39a" class="code"><code>cat &gt; ca-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;Kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;Kubernetes&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF</code></pre><p id="d092819f-c4a9-41f1-ab55-dcdf07ffa72e" class=""><a href="https://docs.microsoft.com/en-us/previous-versions/windows/desktop/ldap/distinguished-names">https://docs.microsoft.com/en-us/previous-versions/windows/desktop/ldap/distinguished-names</a></p><p id="4fc84ef9-03c4-46e6-a22f-3525a818b37f" class="">To generate the CA files after creation of the above files, run: </p><pre id="6f10f1cc-5e42-4753-a05a-90dde4716301" class="code"><code>cfssl gencert -initca ca-csr.json | cfssljson -bare ca</code></pre><p id="20c60f3b-6cd0-40cd-9a4b-f9b0a21e3bd1" class="">The gencert argument generates a private key and a certificate while the -initca parameter is used to initialise the CA. The outcome is two files, the CA certificate and the private key.</p><p id="0b71a07e-bb2f-41da-b6fa-ff6bfcc54d76" class="">
</p><p id="5739dad1-6280-4e91-8f2b-e22f395d481e" class="">After the CA is configured and initilised, create certificates for all Kubernetes components, as well as a client certificate for the admin user. In all cases, the only file that is needed is a CSR that will be used to request a certificate from the CA. Therefore, in each case, the parameters of the cfssl command are the CA certificate, the CA private key, and the CA configuration file while passing as an object the specific component json configuration file.</p><p id="3229b949-adc1-452e-b9e9-0e294824d5a4" class="">
</p><p id="e5c611c0-f499-4a8c-982d-9fa917d03252" class=""><strong>Creating admin user certificate</strong> </p><pre id="f021892b-5ca5-4c1c-b90b-1873b32649b0" class="code"><code>cat &gt; admin-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin</code></pre><p id="f9fb367b-4fbb-4610-9e63-5f052f0f6ffd" class="">Results: admin-key.pem, admin.pem </p><p id="8d929686-0080-4914-81d0-c67dc4f54c2b" class="">
</p><p id="10f36216-6cc6-4d59-84ad-922b704dd67b" class=""><strong>Creating kubelet certificates</strong></p><p id="862fc713-be80-4375-a0c2-d30282b3ed6a" class="">Kubernetes uses a special-purpose authorization mode called Node Authorizer, that specifically authorises API request made by Kubelets. Kubelets must use a credential that identifies them as being in the <mark class="highlight-gray_background">system:nodes</mark> group, with a username of <mark class="highlight-gray_background">system:node:&lt;nodename&gt;</mark>. The common name “system:node:${instance}” refers to the built-in node role in Kubernetes. </p><p id="2d332034-ad9c-4555-8dd1-513e6a6f8ee0" class="">The hostname is a comma separated hostname list that overrides the DNS names and IP address in the certificate SAN, allowing for additional host names to be specified. </p><pre id="c9f290f8-90af-4e4c-801c-6ad74903d2fa" class="code"><code>workers=(kubeworker1 kubeworker2)
declare -A address=(
  [kubeworker1]=10.0.0.5
  [kubeworker2]=10.0.0.6
)


for instance in &quot;${workers[@]}&quot;; d
echo &quot;Updating $instance @ ${address[$instance]}&quot;

cat &gt; ${instance}-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:node:${instance}&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;system:nodes&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${instance},${address[$instance]} \
  -profile=kubernetes \
  ${instance}-csr.json | cfssljson -bare ${instance}

done</code></pre><p id="d458ce98-5a9e-49ba-8c59-7448bee74063" class="">Results: kubeworker1.pem, kubeworker1-key.pem, kubeworker2.pem, kubeworker2-key.pem</p><p id="b864a4a3-412f-4997-bc9b-dd6b65006f1d" class="">
</p><p id="ef3b047c-2028-4fff-b484-743716816e24" class=""><strong>Creating kube-controller-manager certificate</strong> </p><pre id="8ddbf10c-5735-40b9-b605-52b38b22a649" class="code"><code>cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;system:kube-controller-manager&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</code></pre><p id="4af22490-3234-4f8c-a329-ddedfb21f982" class="">Results: kube-controller-manager.pem, kube-controller-manager-key.pem </p><p id="f8ab8b71-34cb-410c-a7ab-a5ecdd16f47f" class="">
</p><p id="79c776ac-a782-4bf9-a18b-c20348360cc8" class=""><strong>Creating kube-proxy certificate</strong> </p><pre id="c3a5847b-70d0-42d8-a3d5-208f809134dc" class="code"><code>cat &gt; kube-proxy-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;system:node-proxier&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy</code></pre><p id="d8382d98-861b-4c4d-9ff0-73128de8a918" class="">Results: kube-proxy.pem, kube-proxy-key.pem </p><p id="904775bc-edcb-4a6d-a068-97c8334deac3" class="">
</p><p id="254ab2b2-38ea-4753-b2e1-7bcafb0635b7" class=""><strong>Creating kube-scheduler certificate</strong> <strong> </strong></p><pre id="65c9d013-d57b-4ee6-8533-f1eaa4afc5af" class="code"><code>cat &gt; kube-scheduler-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:kube-scheduler&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;system:kube-scheduler&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler</code></pre><p id="c0463ed1-d972-4250-871f-e929c48452b3" class="">Results: kube-scheduler-key.pem, kube-scheduler.pem </p><p id="ec5f2e53-c18a-40ee-ace4-af53247c1988" class="">
</p><p id="c1910fb1-e9ec-400b-a31c-9f8fb2dd018b" class=""><strong>Creating kube-api-server certificate</strong> <strong> </strong></p><p id="6c5f31c7-7b57-4638-8898-dafcc9e769e2" class="">The clusters public static IP address will be included in the list of subject alternative names for the Kubernetes API Server certificate. This will ensure the certificate can be validated by remote clients.</p><pre id="40cf0ca8-a3e7-4826-adae-7514c131d9fe" class="code"><code>{

K8_PUBLIC_ADDRESS=10.0.0.1  # set to public IP (load balancer) 
K8_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local
NODES=10.0.0.1,10.0.0.3,10.0.0.4,10.0.0.5,10.0.0.6

cat &gt; kubernetes-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;Kubernetes&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${NODES},${K8_HOSTNAMES},${K8_PUBLIC_ADDRESS},127.0.0.1 \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes

}</code></pre><p id="c22df317-3fe0-4ec1-9bec-1e3907cec524" class="">The Kubernetes API server is automatically assigned the <mark class="highlight-gray_background">kubernetes</mark> internal dns name, which will be linked to the first IP <mark class="highlight-red">address (10.32.0.1) from the address range (10.32.0.0/24) reserved for internal cluster services during the control plane bootstrapping lab.</mark></p><p id="8ce6ce13-18c1-4ca7-b7c6-ff4902204396" class="">Results: kubernetes-key.pem, kubernetes.pem </p><p id="3473b9ab-8b7e-46ef-9fd4-cd7d4493a0eb" class="">
</p><p id="8d156b8c-bf17-49d7-9511-abfb294da0c7" class=""><strong>Creating service account certificate</strong> <strong> </strong></p><p id="a8e6fe39-563a-4db0-ba59-6ae35422b4ca" class="">The Kubernetes Controller Manager leverages a key pair to generate and sign service account tokens.</p><p id="b145af0f-dbdc-434b-ab75-d4b9e781f392" class=""><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/</a></p><pre id="0142246b-c096-41b7-988e-0b266d416188" class="code"><code>cat &gt; service-account-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;service-accounts&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;UK&quot;,
      &quot;L&quot;: &quot;London&quot;,
      &quot;O&quot;: &quot;Kubernetes&quot;,
      &quot;OU&quot;: &quot;K8s Custom&quot;
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account</code></pre><p id="b4030490-de1e-49e2-80be-610383fcc8a6" class="">Results: service-account-key.pem, service-account.pem </p><p id="b11f6470-cffe-483c-b563-c0833d66498e" class="">
</p><p id="233516a3-92cf-4bad-b58c-44db7ad226d8" class=""><strong>Distributing Authentication Certificates and Keys</strong></p><p id="ec23833a-f82b-48f7-8c5e-ad8f2d7e8683" class="">Each worker requires: </p><ul id="9ddce34a-8ebd-4b28-bbe3-3353c95a0a8d" class="bulleted-list"><li>Certification Authority certificate (ca.pem) </li></ul><ul id="16a48209-7c92-4548-92e4-ddde14ab6daa" class="bulleted-list"><li>Worker kubelet certificate (kubeworkerX.pem) </li></ul><ul id="34ce85c1-b1a2-4e2f-a26e-b3a8892ef9e7" class="bulleted-list"><li>Worker kubelet key (kubeworkerX-key.pem) </li></ul><p id="e9d38152-1c22-4500-9fe5-577cf58c1b2a" class="">Each master requires: </p><ul id="0357b67f-6e52-4d48-b9ef-1a1ea2ff14a6" class="bulleted-list"><li>Certification Authority certificate (ca.pem) </li></ul><ul id="6ff38f4b-2c37-4d88-9235-6b20d41c43eb" class="bulleted-list"><li>Certification Authority key (ca-key.pem)</li></ul><ul id="fb91658a-b4e8-478f-9a7b-65dc8cd8e1d2" class="bulleted-list"><li>Kubernetes API certificate (kubernetes.pem) </li></ul><ul id="5a598e5f-7497-4019-a585-464949b800df" class="bulleted-list"><li>Kubernetes API key (kubernetes-key.pem)</li></ul><ul id="c2557caf-1e3c-4c29-b10b-4c871f9da8c8" class="bulleted-list"><li>Service account key-pair certificate (service-account.pem)</li></ul><ul id="628a12af-5541-4cfb-8c41-756431d69881" class="bulleted-list"><li>Service account key-pair key (service-account-key.pem) </li></ul><pre id="31c070dd-b2ae-429a-988f-e7db5976cda9" class="code"><code># worker 
workers=(kubeworker1 kubeworker2)
declare -A address=(
  [kubeworker1]=10.0.0.5
  [kubeworker2]=10.0.0.6
)

for instance in &quot;${workers[@]}&quot;; do
  echo &quot;Updating $instance @ ${address[$instance]}&quot; 
  scp ./certs/ca.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/$instance-key.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/$instance.pem &quot;${address[$instance]}&quot;:~/
done

# master
masters=(kubemaster1 kubemaster2)
declare -A address=(
  [kubemaster1]=10.0.0.3
  [kubemaster2]=10.0.0.4
) 
for instance in &quot;${masters[@]}&quot;; do
  echo &quot;Updating $instance @ ${address[$instance]}&quot;
  scp ./certs/ca.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/ca-key.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/kubernetes-key.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/kubernetes.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/service-account-key.pem &quot;${address[$instance]}&quot;:~/
  scp ./certs/service-account.pem &quot;${address[$instance]}&quot;:~/
done</code></pre><p id="b721a34a-7032-4cb4-a6c2-bd57f8bf59d5" class=""><em>Above script assumes all keys generated in previous steps have been moved to a sub-directory titled </em><em><mark class="highlight-gray_background">certs</mark></em><em>.</em></p><h1 id="a8f158f1-2578-4138-9c7d-ee140bf97d86" class="">Cluster Configuration</h1><p id="fe18449d-f9e1-4627-9d22-6582c55f778c" class="">Kubernetes uses kubeconfig files to organise information about clusters, users, namespaces and authentication mechanisms. The files allow clients, including the <mark class="highlight-gray_background">kubectl</mark> command-line tool to locate and authenticate to the API server of a cluster. </p><p id="02e48b22-6586-4bb2-b452-e46db5d9a00f" class="">By default, kubectl looks for a file named config in the <mark class="highlight-gray_background">$HOME/.kube</mark> directory.</p><p id="648412bc-f4a4-4065-9225-27079b0af00f" class=""><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a></p><p id="271a6de5-cfff-4599-b181-0bb165e99c57" class="">Each kubeconfig requires a Kubernetes API Server to connect to. To support high availability the IP address assigned to the external load balancer fronting the Kubernetes API Servers can be used.</p><p id="4560043f-cabd-45ad-b6a5-5e2120c43fc9" class="">Kubeconfig files are required for: </p><ul id="acf0ad97-0b31-415d-b0ec-7d4bd4b9e392" class="bulleted-list"><li>kube-controller-manager </li></ul><ul id="719b57e1-037b-4a1d-a44b-40e382b01afa" class="bulleted-list"><li>kubelet (one per worker node) </li></ul><ul id="b1215af8-fc7c-4a98-a00f-5797029099b2" class="bulleted-list"><li>kube-proxy </li></ul><ul id="bdee68de-da34-4f35-a217-d33044e654d3" class="bulleted-list"><li>kube-scheduler </li></ul><ul id="829a7962-2e8b-46f9-af4e-e3e009b3fe9e" class="bulleted-list"><li>admin user (kubectl client)</li></ul><p id="addf2724-1ac0-4d03-a807-aedf85afc306" class="">
</p><p id="5c60f3aa-7ad4-45f0-924c-2b13cfe22b69" class=""><strong>kubelet configuration file</strong></p><p id="89c59f44-f2a6-4d34-a469-bb0ac462e29b" class="">When generating kubeconfig files for Kubelets the client certificate matching the Kubelet&#x27;s node name must be used.</p><pre id="6318afd2-7667-467d-901a-26323cc18237" class="code"><code>KUBERNETES_PUBLIC_ADDRESS=10.0.0.1 # set to cluster public IP

for instance in kubeworker1 kubeworker2; do
  kubectl config set-cluster k8s_cluster \
    --certificate-authority=./certs/ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=./certs/${instance}.pem \
    --client-key=./certs/${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=k8s_cluster \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done</code></pre><p id="3a0255cd-b60c-4553-8cd3-e496c7efbbbf" class="">Results: kubeworker1.kubeconfig, kubeworker2.kubeconfig </p><p id="61bb78c5-1d6b-41aa-9766-eb77cf705234" class="">
</p><p id="5019aebc-65c3-494b-9edd-c16891946249" class=""><strong>kube-proxy configuration file</strong></p><pre id="c32c22bc-b920-423f-9c9d-f5f5a07008af" class="code"><code>KUBERNETES_PUBLIC_ADDRESS=10.0.0.1 # set to cluster public IP

{
  kubectl config set-cluster k8s_cluster \
    --certificate-authority=./certs/ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=./certs/kube-proxy.pem \
    --client-key=./certs/kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-context default \
    --cluster=k8s_cluster \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
}</code></pre><p id="cb4cb821-0cd5-4411-ae62-a7f914b663c0" class="">Results: kube-proxy.kubeconfig </p><p id="60d3a711-0a9a-4443-bbe4-a30445c62d6f" class="">
</p><p id="0f847c0c-387a-40ce-9498-edb733681e9c" class=""><strong>kube-controller-manager configuration file </strong></p><pre id="1bdcc23c-6737-4dc9-a15c-39d5620a44dc" class="code"><code>KUBERNETES_PUBLIC_ADDRESS=10.0.0.1 # set to cluster public IP

{
  kubectl config set-cluster k8s_cluster \
    --certificate-authority=./certs/ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=./certs/kube-controller-manager.pem \
    --client-key=./certs/kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config set-context default \
    --cluster=k8s_cluster \
    --user=system:kube-controller-manager \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
}</code></pre><p id="c693def2-1bb4-4c6d-9251-5464e6757655" class="">Results: kube-controller-manager.kubeconfig </p><p id="c0de6173-fbd4-4486-9f24-35e1e1833f8a" class="">
</p><p id="bdb3364d-a1b3-4759-8604-9be10a325a5e" class=""><strong>kube-scheduler configuration file </strong></p><pre id="b72b3ba6-55ee-449b-b4b9-481c20e4f617" class="code"><code>KUBERNETES_PUBLIC_ADDRESS=10.0.0.1 # set to cluster public IP

{
  kubectl config set-cluster k8s_cluster \
    --certificate-authority=./certs/ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=./certs/kube-scheduler.pem \
    --client-key=./certs/kube-scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-context default \
    --cluster=k8s_cluster \
    --user=system:kube-scheduler \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
}</code></pre><p id="e1e0803d-f928-4bd2-be14-6a4ebddf4516" class="">Results: kube-scheduler.kubeconfig </p><p id="7dbb5084-b457-4f0d-baaf-bd7ca5c535d7" class="">
</p><p id="4bece083-85c1-4bb0-95a9-456c1575f478" class=""><strong>admin configuration file </strong></p><pre id="6eb80f4f-8bc4-49c3-b79e-ea1e05e03988" class="code"><code>KUBERNETES_PUBLIC_ADDRESS=10.0.0.1 # set to cluster public IP

{
  kubectl config set-cluster k8s_cluster \
    --certificate-authority=./certs/ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=admin.kubeconfig

  kubectl config set-credentials admin \
    --client-certificate=./certs/admin.pem \
    --client-key=./certs/admin-key.pem \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

  kubectl config set-context default \
    --cluster=k8s_cluster \
    --user=admin \
    --kubeconfig=admin.kubeconfig

  kubectl config use-context default --kubeconfig=admin.kubeconfig
}</code></pre><p id="eba61a2b-6645-44ce-8591-ccb790548269" class="">Results: admin.kubeconfig </p><p id="f0736e38-766d-497d-8e42-da7e08e1b39d" class="">
</p><p id="8f78b966-1f95-4d7c-9dd1-4545dccbbd9f" class=""><strong>Distribute the Kubernetes Configuration Files </strong></p><p id="8c5b8ff2-8fb0-405f-82c7-d942e4d68409" class="">Each worker requires: </p><ul id="73f0f0c4-7088-4f62-8c8b-30245b263387" class="bulleted-list"><li>kubelet config file (kubeworkerX.kubeconfig)</li></ul><ul id="849a99ed-a42e-44c4-ba71-11b3c1920ddd" class="bulleted-list"><li>kube-proxy config file (kube-proxy.kubeconfig)</li></ul><p id="29015fcb-5c1d-4f6d-84ac-79c715a18fa1" class="">Each master requires: </p><ul id="a3072853-1cff-4b74-a284-c40412a88872" class="bulleted-list"><li>kube-controller-manager config file (kube-controller-manager.kubeconfig)</li></ul><ul id="b878a08a-167e-4e84-9b38-b08b6139bd4e" class="bulleted-list"><li>kube-scheduler config file (kube-scheduler.kubeconfig)</li></ul><ul id="e1a287b7-9c60-4946-8a8d-de6c72855751" class="bulleted-list"><li>admin config file (admin.kubeconfig) </li></ul><pre id="3cd187b0-50a6-4b70-a14f-6df1cd6a09ee" class="code"><code># worker 
workers=(kubeworker1 kubeworker2)
declare -A address=(
  [kubeworker1]=10.0.0.5
  [kubeworker2]=10.0.0.6
)

for instance in &quot;${workers[@]}&quot;; do
  echo &quot;Updating $instance @ ${address[$instance]}&quot; 
  scp ./configs/kube-proxy.kubeconfig &quot;${address[$instance]}&quot;:~/
  scp ./configs/$instance.kubeconfig &quot;${address[$instance]}&quot;:~/
done

# master 
masters=(kubemaster1 kubemaster2)
declare -A address=(
  [kubemaster1]=10.0.0.3
  [kubemaster2]=10.0.0.4
)
for instance in &quot;${masters[@]}&quot;; do
  echo &quot;Updating $instance @ ${address[$instance]}&quot;
  scp ./configs/admin.kubeconfig &quot;${address[$instance]}&quot;:~/
  scp ./configs/kube-controller-manager.kubeconfig &quot;${address[$instance]}&quot;:~/
  scp ./configs/kube-scheduler.kubeconfig &quot;${address[$instance]}&quot;:~/
done</code></pre><p id="4adeb1e1-e02f-4992-a055-7ed7368d20e4" class=""><em>Above script assumes all keys generated in previous steps have been moved to a sub-directory titled </em><em><mark class="highlight-gray_background">configs</mark></em><em>.</em></p><p id="bd59e4ec-c335-4fda-9dbf-ce3840e87aef" class="">
</p><h1 id="7544931d-1c80-41b6-83f2-48fe6f86cf8a" class="">Cluster Data Encryption</h1><p id="e688d39f-0ac6-47fc-b4c9-d077ad48e8bb" class="">Kubernetes stores a variety of data including cluster state, application configurations, and secrets. In this project, data is stored in the etcd cluster. Kubernetes supports the ability to encrypt cluster data at rest using an encryption config. </p><p id="6653d022-5d21-4d03-8bec-ef62b0555bb1" class=""><a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#understanding-the-encryption-at-rest-configuration">https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#understanding-the-encryption-at-rest-configuration</a></p><pre id="ec608bbc-af89-401a-9fb5-98b115a2fd0d" class="code"><code>ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

cat &gt; encryption-config.yaml &lt;&lt;EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF</code></pre><p id="ba3b551d-fbcf-47fc-a997-4db4eb25d89b" class="">Result: encryption-config.yaml </p><p id="e06a3ea9-b532-493e-bbb9-488696b0bd3b" class="">The encryption-config.yaml encryption config file needs to be copied to each master instance. </p><pre id="c09c2e72-49e7-4756-bc27-0633d403b26d" class="code"><code>masters=(kubemaster1 kubemaster2)
declare -A address=(
  [kubemaster1]=10.0.0.3
  [kubemaster2]=10.0.0.4
)
for instance in &quot;${masters[@]}&quot;; do
  scp ./configs/encryption-config.yaml &quot;${address[$instance]}&quot;:~/
done</code></pre><h1 id="5f1c60ce-aaef-4697-b946-5fb82ad93067" class="">Bootstrapping the ectd Cluster </h1><p id="824274b7-6866-405c-ba13-0eb29bfbfb4d" class="">Kubernetes components are stateless and store cluster state data in etcd. The ectd can be provisioned on independent nodes for high-availability. This project will use as stack topology, with ectd cluster instances located within each master node.  </p><p id="2bf19b4d-3876-4667-9816-8e2cefa06c7f" class="">The configuration for this section must be run for each master instance: <mark class="highlight-gray_background">kubemaster1</mark>, <mark class="highlight-gray_background">kubemaster2</mark> via SSH. </p><ol id="b7a482c9-9807-4b55-b428-10f4cfc8754e" class="numbered-list" start="1"><li>Download official etcd release binaries from GitHub project:<pre id="d0141c5e-65a7-4889-beea-234132315dd8" class="code"><code>$ wget -q --show-progress --https-only --timestamping \
  &quot;https://github.com/etcd-io/etcd/releases/download/v3.4.0/etcd-v3.4.0-linux-amd64.tar.gz&quot;</code></pre></li></ol><ol id="ccf92945-da26-4d76-822b-28dcd648afb4" class="numbered-list" start="2"><li>Extract and install the ectd server and the etcdctl command line utility <pre id="4af59c15-455c-44b1-b7c4-76d41f7c3302" class="code"><code>$ tar -xvf etcd-v3.4.0-linux-amd64.tar.gz
$ rm -r etcd-v3.4.0-linux-amd64.tar.gz
$ sudo mv etcd-v3.4.0-linux-amd64/etcd* /usr/local/bin/
$ rm -r etcd-v3.4.0-linux-amd64/</code></pre></li></ol><ol id="f09719d4-3712-4228-bfd9-71b4729b8b9d" class="numbered-list" start="3"><li>Configure the etcd server (run from home/dev/ or ~/) <pre id="4b2ee745-e15d-44ac-bb70-cf3958d52e4a" class="code"><code>$ sudo sudo mkdir -p /etc/etcd /var/lib/etcd
$ sudo sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/</code></pre></li></ol><ol id="57a6fdbb-c890-4fd3-b474-4b91d3594df3" class="numbered-list" start="4"><li>The instance internal IP address will be used to serve client requests and communicate with etcd cluster peers. Each etcd member must have a unique name within an etcd cluster. <pre id="4f66fe28-3597-4252-a57d-704e5be8a027" class="code"><code>$ INTERNAL_IP=10.0.0.3 # internal IP for instance
$ ETCD_NAME=$(hostname -s)etcd</code></pre></li></ol><ol id="3bff12d6-3821-4864-9714-7222a886e34c" class="numbered-list" start="5"><li>Create the etcd.service systemd unit file <pre id="7b4ceae0-d7e0-40f2-8cfb-0482025f8b6b" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster kubemaster1etcd=https://10.0.0.3:2380,kubemaster2etcd=https://10.0.0.4:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre><p id="a39e1ed4-67bf-49db-bd7d-3588cfa7018d" class="">
</p></li></ol><ol id="6f21b90e-16ab-42b7-ba78-7eaf1ec0f770" class="numbered-list" start="6"><li>Start the etcd server <pre id="56f33352-6938-4148-8ea2-dea45046e9c8" class="code"><code>$ sudo systemctl daemon-reload
$ sudo systemctl enable etcd
$ sudo systemctl start etcd</code></pre></li></ol><ol id="9bfac0da-0173-4ff1-b6b2-cf274375672b" class="numbered-list" start="7"><li>Verification, list the etcd cluster members<pre id="fb31ab91-c919-43e1-8cf4-18b00e656461" class="code"><code>sudo ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem</code></pre><p id="78e48492-9a7f-4741-983d-1bd731261716" class="">Expected output: </p><pre id="a6020006-602c-47b4-804f-4bca17053523" class="code"><code>3a57933972cb5131, started, kubemaster1, https://10.0.0.3:2380, https://10.0.0.3:2379
f98dc20bce6225a0, started, kubemaster2, https://10.0.0.4:2380, https://10.0.0.4:2379</code></pre></li></ol><p id="13531670-e05d-4975-a22d-2c74d045646f" class="">
</p><h1 id="0d090fd1-8102-431d-ab3d-49cf3f45d1b2" class="">Bootstrapping the Control Plane</h1><p id="fc48b51a-3a71-4324-bf95-03f43eddf53b" class="">This section will bootstrap the Kubernetes control plane across the compute instance(s) and configure it for high availability. </p><p id="a5911291-8dae-4ae9-b466-1efa994e5e71" class="">The configuration for this section must be run for each master instance: <mark class="highlight-gray_background">kubemaster1</mark>, <mark class="highlight-gray_background">kubemaster2</mark> via SSH. </p><ol id="a4f7ca09-9e5f-4ee9-acbf-5dd011ee3f36" class="numbered-list" start="1"><li>Create the Kubernetes configuration directory <pre id="37c62451-5b2f-4ae2-b8e0-29067e3427a1" class="code"><code>sudo mkdir -p /etc/kubernetes/config</code></pre></li></ol><ol id="32985a3b-c50f-432b-9736-c00776961059" class="numbered-list" start="2"><li>Download the official kube-apiserver, kube-controller-manager, kube-scheduler and kubectl binaries. 
<mark class="highlight-red">Pull from git?</mark><pre id="97f2a922-d14e-441a-a8da-1ca2938da363" class="code"><code>$ wget -q --show-progress --https-only --timestamping \
  &quot;https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kube-apiserver&quot;
$ wget -q --show-progress --https-only --timestamping \
  &quot;https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kube-controller-manager&quot;
$ wget -q --show-progress --https-only --timestamping \
  &quot;https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kube-scheduler&quot;
$ wget -q --show-progress --https-only --timestamping \
  &quot;https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kubectl&quot;</code></pre></li></ol><ol id="44f43246-4e7b-418d-9399-0706a2278b9f" class="numbered-list" start="3"><li>Install downloaded binaries <pre id="26b94ea7-266e-452e-ad23-74c9b018f363" class="code"><code>$ chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
$ sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/</code></pre></li></ol><p id="cc5ceaa6-863c-4cb5-a750-e307d94acdc8" class="">
</p><p id="2e07ca18-d7f9-4691-876a-66ad789ef531" class=""><strong>Kubernetes API Sever Configuration</strong></p><ol id="655498d9-8be1-4d9a-b6fd-0639b39a4c55" class="numbered-list" start="1"><li>Configure the API server (run from /home/dev or ~/) <pre id="adf10b37-ecd3-4fdc-96b7-a0e57f891e19" class="code"><code>sudo mkdir -p /var/lib/kubernetes/

sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem \
    encryption-config.yaml /var/lib/kubernetes/</code></pre></li></ol><ol id="b7c0c3e3-b7ce-4af2-be07-4f00d92d645d" class="numbered-list" start="2"><li>The instance internal IP address will be used to advertise the API Server to members of the cluster. Retrieve the internal IP address for the current compute instance
<mark class="highlight-red">Note: service-cluster-ip-range defined (search 10.32.0.1, related to DNS assignment of &#x27;kubernetes&#x27;) </mark><pre id="f22acd8b-c388-4f42-85e7-505619d9662e" class="code"><code>INTERNAL_IP=10.0.0.3 # set to internal IP of node 

cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://10.0.0.3:2379,https://10.0.0.4:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-https=true \\
  --runtime-config=api/all \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre></li></ol><p id="11cc2b91-c913-46a5-beed-ed6f6ac45421" class="">
</p><p id="c9a4deef-ab68-4927-b5d7-1d796bad6130" class=""><strong>Kubernetes Controller Manager Configuration</strong></p><ol id="7735e8dc-73a9-4c58-83a8-0b6d6ef5da23" class="numbered-list" start="1"><li>Move the kubeconfig into place (run from /home/dev or ~/) <pre id="3edf342e-1016-47dd-b993-fe8731caaff5" class="code"><code>sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/</code></pre></li></ol><ol id="7936f0d5-89d5-4ae3-b642-adb88550ec84" class="numbered-list" start="2"><li>Create the kube-controller-manager.service systemd unit file 
<mark class="highlight-red">Note: cluster-cidr and service-cluster-ip-range defined</mark><pre id="39098a9c-62b0-4435-b885-2ca3ee4d1e27" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=k8s_cluster \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre></li></ol><p id="8a728a43-949e-468b-a4fe-558c689904db" class="">
</p><p id="65ee9495-7eb2-4258-9259-00fd7b37cdef" class=""><strong>Kubernetes Scheduler Configuration </strong></p><ol id="a6558a26-91a8-4a79-9cef-e0c9c6b05dbf" class="numbered-list" start="1"><li>Move the kubeconfig into place (run from /home/dev or ~/) <pre id="bf3a9bcd-3b9b-42f9-bebb-feee8eb794de" class="code"><code>sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/</code></pre></li></ol><ol id="57d90b3d-efcf-48f1-91a6-a298b280d9cb" class="numbered-list" start="2"><li>Create the kube-scheduler.yaml configuration file<pre id="8f33e9e7-af41-48d6-bfc2-ad5c936d263e" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: &quot;/var/lib/kubernetes/kube-scheduler.kubeconfig&quot;
leaderElection:
  leaderElect: true
EOF</code></pre></li></ol><ol id="deb8fe21-91c2-4b06-9e5e-b03ad7fc1cda" class="numbered-list" start="3"><li>Create the kube-scheduler.service systemd unit file <pre id="6e3a116b-5f2c-46c2-bd4c-87e454496332" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre></li></ol><p id="08ffbd54-632a-4923-acec-9ad866614b90" class="">
</p><p id="dfc9f09b-5b15-4297-b240-be206d961cc5" class=""><strong>Start Controller Services</strong></p><pre id="18fbf6fd-f235-4214-a19a-c8ad5a425a96" class="code"><code>$ sudo systemctl daemon-reload
$ sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
$ sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler</code></pre><p id="15517920-ee63-4a4c-9879-7579ba9404e4" class="">
</p><p id="703c36ec-62ba-4c8d-9ecf-f607e1ef142a" class=""><strong>Verification</strong></p><p id="1a111b75-b9b8-43d9-8833-acedabefdc12" class="">Run from a master node. </p><pre id="93e9d10c-566b-4e8f-8a4a-7777875f78b4" class="code"><code>$ kubectl get componentstatuses --kubeconfig admin.kubeconfig

# Expected response 
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}</code></pre><p id="6c7dc040-ddcf-47f3-9e63-707588e62218" class="">Health check queries may fail if router through some types of load balancers that do not support HTTPS health checks. An intermediate server can solve the problem.. see: </p><p id="7425e5c7-4878-4f1d-b4df-fca106616f2f" class=""><a href="https://github.com/DanielSCrouch/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md">https://github.com/DanielSCrouch/kubernetes-the-hard-way/blob/master/docs/08-bootstrapping-kubernetes-controllers.md</a></p><h1 id="fd915e87-482d-4a7b-b46a-92e7f3f69de1" class="">RBAC for Kubelet Authorisation </h1><p id="b8468837-939b-4c90-ae2c-6e08614a22da" class="">Configure RBAC permissions to allow the Kubernetes API Server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs, and executing commands in pods.</p><p id="fdc8cb49-a280-49d2-83c5-5f2062e0215e" class="">This section sets the Kubelet --authorization-mode flag to Webhook. Webhook mode uses the SubjectAccessReview API to determine authorization.</p><p id="5f629195-0d0c-4c6b-a18a-0558e60dda6d" class=""><a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access">https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access</a></p><p id="328a00f2-b5f7-4198-a915-3da03c06ba08" class="">The commands in this section will effect the entire cluster and only need to be run once from one of the controller nodes.</p><ol id="bc6fa7d0-48b2-45c4-9688-5691b537dce3" class="numbered-list" start="1"><li>Create the system:kube-apiserver-to-kubelet ClusterRole with permissions to access the Kubelet API and perform most common tasks associated with managing pods<pre id="fa211f97-436b-4ad0-9cde-d2764699251d" class="code"><code>cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - &quot;&quot;
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - &quot;*&quot;
EOF</code></pre><p id="290e35a3-2813-4d15-8bc2-ffdd562d0ced" class="">The Kubernetes API Server authenticates to the Kubelet as the kubernetes user using the client certificate as defined by the --kubelet-client-certificate flag.</p></li></ol><ol id="430775c9-780e-42e5-879c-7bd44c35835c" class="numbered-list" start="2"><li>Bind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user<pre id="2e0b6bb7-372a-48c5-ace6-eded1b03e0be" class="code"><code>cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: &quot;&quot;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF</code></pre></li></ol><p id="234298fb-57c1-4519-a316-3b10b6ed6772" class="">
</p><h1 id="eda4f99f-2920-4b2d-9f8c-3bf8b858ad21" class="">Frontend Load Balancer </h1><p id="21f689a6-35f4-4f8b-822f-64ccda5c9c6c" class="">This section will provision an external load balancer to direct traffic to the Kubernetes cluster API Servers. </p><p id="e91c6f18-9c07-42e5-bf10-5d1c913a362c" class="">Setup specific: </p><p id="f1306070-0d4f-4827-867a-b5a9ac6d4cc9" class="">The host machine, and the four Ubuntu Server nodes reside on the same VirtualBox &#x27;host-only&#x27; network (10.0.0.0/24) with the host machine statically assigned 10.0.0.1. </p><p id="6a357942-353d-4b09-8bac-1e92aa68d5e2" class="">Throughout the setup steps so far, the cluster&#x27;s public IP to the API Sever has been assigned to 10.0.0.1 for Development convenience (its static, locally controlled and on the same network).  </p><p id="4ebf2e8c-58bf-4618-81b9-85a60073af54" class="">The worker nodes also communicate with the API Server through this public address to ensure requests are load-balanced. </p><p id="b665c7ef-6892-4e06-b026-73d6318c670e" class="">Requests inbound to the cluster are handled by either of the API Server instances running on the master nodes. The public IP must be handled by a proxy (or load balancer), to direct API requests from the clusters public IP (10.0.0.1) to either of the API servers (10.0.0.3 or 10.0.0.4).</p><p id="6f9fd84f-2f22-49a3-8ad6-955b81c108fb" class="">A HA Proxy server will be used to load balance the ingress traffic on 10.0.0.1 to the API servers running on master nodes 10.0.0.3 and 10.0.0.4. The HA Proxy software will be managed through a Docker container running on the host OS.  </p><p id="5974cac8-abbc-4237-8f0d-3783299a8ec8" class=""><strong>Docker Installation </strong></p><p id="e0e6486a-6c98-4879-bb9f-e5e6b1d93abe" class="">The HA Proxy can run directly on the OS. If using a containerised version (as done here), then Docker must be installed on the host OS. </p><p id="0b161b21-6c99-43b4-9b15-5eb9c17ce0ac" class=""><a href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a></p><p id="35515296-6a16-4f77-a7b0-fa291895acad" class=""><strong>HA Proxy Configuration </strong></p><p id="ef505815-e22c-43b6-a80c-f4fea4ec7481" class="">An open source, high availability load balancer, and TCP, HTTP proxy server. </p><p id="0a82286d-28ce-4b48-a6c2-444cb47b56b1" class=""><a href="https://hub.docker.com/_/haproxy">https://hub.docker.com/_/haproxy</a></p><p id="02d97057-365e-4354-85ab-0ec8baa8ea4e" class=""><a href="https://www.linode.com/docs/uptime/loadbalancing/how-to-use-haproxy-for-load-balancing/">https://www.linode.com/docs/uptime/loadbalancing/how-to-use-haproxy-for-load-balancing/</a></p><p id="2c419ea4-7a95-4b66-b19e-ca8325123240" class="">The behavior of HA Proxy is determined by a configuration file titled <mark class="highlight-gray_background">haproxy.cfg</mark>, located at <mark class="highlight-gray_background">/usr/local/etc/haproxy/</mark> on the standard docker instance.</p><p id="36b81d1e-fb14-4417-b27f-2f1b1968e7cf" class="">The key sections here are the &#x27;Frontend server&#x27; and &#x27;Backend server&#x27;. The former binds to port 6443 (Kubernetes API Server Port) and redirects traffic to the backend. The backend distributes the inbound requests (using round robin scheduling) to the two master nodes. </p><pre id="8d52f5d3-5c4a-44e1-8c3f-f7bb519caab9" class="code"><code>#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    log         127.0.0.1 local2
    pidfile     /var/run/haproxy.pid
    maxconn     4096
    nbproc      4
    daemon

#---------------------------------------------------------------------
# Common defaults that all the &#x27;listen&#x27; and &#x27;backend&#x27; sections will 
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode            http
    log             global
    option          dontlognull
    option          httpclose
    option          httplog
    option          forwardfor
    option          redispatch
    timeout connect 10000 # default 10 second time out if a backend is not found
    timeout client  50000
    timeout server  50000
    maxconn         60000
    retries         3

#---------------------------------------------------------------------
# Frontend server 
#---------------------------------------------------------------------
frontend k8s 
    bind *:6443
    mode tcp 
    default_backend k8s

#---------------------------------------------------------------------
# Backend server(s) 
#---------------------------------------------------------------------
backend k8s    
    balance roundrobin 
    mode tcp 
    option tcplog 
    option tcp-check 
    server kubemaster1 10.0.0.3:6443 check 
    server kubemaster2 10.0.0.4:6443 check</code></pre><p id="fe9fc633-8294-4854-8a80-1df90a20d13d" class="">
</p><p id="4f53c3e3-4f13-47dd-9751-128d10d7068b" class=""><strong>Building HA Container  </strong></p><p id="0a2e30ba-21c0-4563-af57-39b76ceff920" class="">To build the HA_Proxy container on the host machine a <mark class="highlight-gray_background">Dockerfile</mark> (Docker configuration file) is required. The file specifies the image (haproxy, version 1.7) to be pulled from DockerHub. The copy declaration duplicates the configuration file into the default location within the container. </p><pre id="27538a1a-adc1-43e6-95cf-48d4247724c2" class="code"><code>FROM haproxy:1.7 
COPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg</code></pre><p id="d0cb5d19-44f0-4a20-94b1-c7e8d9eb862b" class="">To build the HA Proxy container image (here named k8s), from the command line:</p><pre id="9c6af028-6ace-4999-a889-6162c425fe85" class="code"><code>$ docker build -t k8s . </code></pre><p id="e59f2d85-ab9b-429c-9db6-531bc30d62b2" class="">
</p><p id="684d61bb-8a43-4b31-94f7-ac3564667221" class=""><strong>Launch Load Balancer (HA Proxy)</strong></p><p id="47293ead-0253-44e6-a9bd-822ac3b2c4db" class="">As covered in the section introduction, the aim of the load balancer is to direct traffic intended for the cluster&#x27;s API Server on address 10.0.0.1:6443 to the master nodes. The forwarding logic is covered by the haproxy.cfg file and implemented by HA Proxy within the container.  To run the container: </p><pre id="15bf8c75-a863-4007-bf3e-31cdc7f08c0b" class="code"><code>$ docker run -d -p 6443:6443 --ip=10.0.0.1 --name k8s-run k8s</code></pre><p id="3c403456-3f36-419a-a258-d68f9e3740fa" class="">In order to direct inbound requests from the hosts IP (10.0.0.1:6443) to the container the containers ports must be mapped (-p 6443:6443) and the source IP specified (--ip=10.0.0.1).</p><p id="b708b578-c4cc-4d19-9665-f0ad6da04059" class="">At this stage the host machine has multiple interfaces (see ifconfig); including the WAN router, VirtualBox host-only and Docker (usually 172.17.0.1). </p><p id="bf5df0eb-7bb7-483c-82a0-dfe2f4bdf239" class="">
</p><p id="4595cb0d-f70a-4002-9c0b-84ac95cecbae" class=""><strong>Verification</strong></p><p id="e03c46c3-fc1e-480b-be43-1ec8fa4ba7d6" class="">Check the container is running:</p><pre id="6523569e-22ca-4f93-8a88-224d62e0485a" class="code"><code>$ docker ps 
# Expected output 
CONTAINER ID   IMAGE  COMMAND                 CREATED         STATUS         PORTS                    NAMES
35f421aef52d   k8s    &quot;/docker-entrypoint.…&quot;  29 minutes ago  Up 29 minutes  0.0.0.0:6443-&gt;6443/tcp   k8s-run</code></pre><p id="aa684e5e-5290-42aa-a7a1-92c20435502a" class="">If the container is not running, check the containers logs  </p><pre id="51636395-b49c-4033-8be1-010ad804fc83" class="code"><code>$ docker logs k8s-run </code></pre><p id="a2887402-2a36-4f71-acda-e2e48b680ef8" class="">Confirm the host&#x27;s 6443 port is open (run from any node)  </p><pre id="b5d18e76-d44d-4ab2-a275-ada4dc994f3c" class="code"><code>$ nmap -p 6443 10.0.0.1
# Expected output 
Host is up (0.00025s latency).
PORT     STATE SERVICE
6443/tcp open  sun-sr-https</code></pre><p id="768c5efd-4478-4286-ba24-b2a5372483a3" class="">To access the container&#x27;s shell directly </p><pre id="e07cd8a8-3f8a-4609-8e1e-f82156e0d2a3" class="code"><code>$ docker exec -it k8s-run bash 
# Execpted output (container shell, type exit) 
root@35f421aef52d:/#</code></pre><p id="91b74b5b-f8c5-4c60-8a2f-0acda69e1c15" class="">Verify external Kubernetes API requests</p><p id="c89371e8-0c0a-45df-ae4e-7c2e8ecec903" class="">Run command from any node, providing current working directory has a copy of ca.pem key (i.e. host, or master in /var/lib/kubernetes) </p><p id="597c1bc5-71c7-4bcb-9b52-01e0f0b52ff2" class="">Verify external Kubernetes API Health requests</p><pre id="e529508e-5d76-4c16-8b8f-45ae853c4a63" class="code"><code>$ curl -H &quot;Host: kubernetes.default.svc.cluster.local&quot; -i http://10.0.0.1:80</code></pre><p id="3b352312-3d81-40d3-8a6a-1368052976fe" class="">
</p><h1 id="03d3ef3d-c606-4662-bd53-395bec0e6adc" class="">Bootstrapping the Workers</h1><p id="885476ea-3931-4631-9abf-11c0fd429564" class="">The following components will be installed on each node: </p><ul id="a845e261-6656-47dd-be59-f4a093dfdf57" class="bulleted-list"><li>Docker container runtime application </li></ul><ul id="330d93de-6e91-4268-848e-27ace1b47a3f" class="bulleted-list"><li>Kubelet </li></ul><ul id="9a5d0119-c106-4be7-8642-8ee22c773c44" class="bulleted-list"><li>Kube-proxy </li></ul><ul id="bfca89ae-a79f-4b4f-8c5d-43195764f268" class="bulleted-list"><li>Runc ????</li></ul><ul id="6e06d5ba-e788-43f5-8f85-2ffa7e0546cb" class="bulleted-list"><li>Container networking plugins????</li></ul><ul id="f83f671a-9675-4b06-a70e-df65745cc11e" class="bulleted-list"><li>Flannel </li></ul><p id="e8abe983-f835-4b6e-97f4-1cb0a8edd937" class="">
</p><p id="619c41e6-ed40-48cd-9623-56ed41d00f7e" class=""><strong>OS Dependencies </strong></p><pre id="cdb70059-236a-4d2e-ba82-3be58ee38370" class="code"><code>$ sudo apt-get update
$ sudo apt-get -y install socat conntrack ipset</code></pre><p id="72e29988-171f-48bb-b7eb-a6e76ed67cac" class="">
</p><p id="790ba642-25e0-4c51-8168-3e8607c1e037" class=""><strong>Disabling Swap </strong></p><p id="aff1b51e-6a7a-4039-8b3c-a7e7821e7acf" class="">By default the kubelet will fail to start if swap is enabled. It is recommended that swap be disabled to ensure Kubernetes can provide proper resource allocation and quality of service.</p><pre id="8ca342a7-f1e0-4498-8989-8fd9c8620b53" class="code"><code>$ sudo swapon --show # verify, if empty then not enabled
$ sudo swapoff -a    # disable swap space</code></pre><p id="3f7777c4-25fb-404a-a8ec-f6278d79585f" class="">To disable swap permanently, comment out swapfile or swap entry in /etc/fstab</p><p id="a45fead2-05c7-4b97-a3fd-42b6bc1aff4d" class="">
</p><p id="bc494a90-35e3-49bf-b9fc-b4ff4c134414" class=""><strong>Kubernetes Component Installation  </strong></p><pre id="f45e3056-1a60-4b2d-a147-b35b5186c07a" class="code"><code>wget -q --show-progress --https-only --timestamping \
  https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kubelet

sudo mkdir -p \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes

chmod +x kubectl kube-proxy kubelet 
sudo mv kubectl kube-proxy kubelet /usr/local/bin/</code></pre><p id="e6b08820-d070-44a4-b29f-4080ffcb939e" class="">
</p><h2 id="82b68843-4dcd-4bdf-93e9-86b56fe6f96f" class=""><mark class="highlight-teal">CHECKPOINT</mark> </h2><p id="65f3ef67-2250-4183-a25e-0142be1d1175" class=""><strong>Container Dependencies </strong></p><pre id="bcc55793-4748-4d7b-b55c-969c8135a8fd" class="code"><code>$ sudo wget -q --show-progress --https-only --timestamping \
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.15.0/crictl-v1.15.0-linux-amd64.tar.gz \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc8/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.8.2/cni-plugins-linux-amd64-v0.8.2.tgz \
  https://github.com/containerd/containerd/releases/download/v1.2.9/containerd-1.2.9.linux-amd64.tar.gz

$ sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin

$ mkdir containerd
$ tar -xvf crictl-v1.15.0-linux-amd64.tar.gz
$ tar -xvf containerd-1.2.9.linux-amd64.tar.gz -C containerd
$ sudo tar -xvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/
$ sudo mv runc.amd64 runc
$ sudo chmod +x crictl runc 
$ sudo mv crictl runc /usr/local/bin/
$ sudo mv containerd/bin/* /bin/

$ rm cni-plugins-linux-amd64-v0.8.2.tgz
$ rm containerd-1.2.9.linux-amd64.tar.gz
$ rm crictl-v1.15.0-linux-amd64.tar.gz
$ rm -r containerd/</code></pre><p id="5ccac7ba-6369-4279-99ff-c200db3003b5" class="">
</p><p id="998eadbb-0d8f-48b1-8c2f-ca0afd545f88" class=""><strong>Configure CNI Networking </strong></p><pre id="30602970-f551-40e7-a93a-a3b26c63c024" class="code"><code>$ POD_CIDR=10.200.1.0/24 # for first worker, increment 3rd byte 

cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;bridge&quot;,
    &quot;type&quot;: &quot;bridge&quot;,
    &quot;bridge&quot;: &quot;cnio0&quot;,
    &quot;isGateway&quot;: true,
    &quot;ipMasq&quot;: true,
    &quot;ipam&quot;: {
        &quot;type&quot;: &quot;host-local&quot;,
        &quot;ranges&quot;: [
          [{&quot;subnet&quot;: &quot;${POD_CIDR}&quot;}]
        ],
        &quot;routes&quot;: [{&quot;dst&quot;: &quot;0.0.0.0/0&quot;}]
    }
}
EOF

cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;lo&quot;,
    &quot;type&quot;: &quot;loopback&quot;
}
EOF</code></pre><p id="932ee620-5cad-4583-976e-f56f56de6353" class="">
</p><p id="c3c269e1-5fc3-4354-ba45-fb18e59d5c15" class=""><strong>Configure Containerd</strong></p><pre id="b69b3a7c-b596-4a64-99ee-745183c4f112" class="code"><code>sudo mkdir -p /etc/containerd/

cat &lt;&lt; EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = &quot;overlayfs&quot;
    [plugins.cri.containerd.default_runtime]
      runtime_type = &quot;io.containerd.runtime.v1.linux&quot;
      runtime_engine = &quot;/usr/local/bin/runc&quot;
      runtime_root = &quot;&quot;
EOF

cat &lt;&lt;EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF</code></pre><p id="f9eaffc6-4761-4772-b7ab-6fb9e2b55079" class="">
</p><p id="4e7dea8f-d284-4910-b304-d273fd172296" class=""><strong>Configure the Kubelet </strong></p><pre id="64c01ad2-feb6-4335-a753-e2fc4096d86a" class="code"><code>$ sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
$ sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
$ sudo mv ca.pem /var/lib/kubernetes/

$ cat &lt;&lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: &quot;/var/lib/kubernetes/ca.pem&quot;
authorization:
  mode: Webhook
clusterDomain: &quot;cluster.local&quot;
clusterDNS:
  - &quot;10.32.0.10&quot;
podCIDR: &quot;${POD_CIDR}&quot;
resolvConf: &quot;/run/systemd/resolve/resolv.conf&quot;
runtimeRequestTimeout: &quot;15m&quot;
tlsCertFile: &quot;/var/lib/kubelet/${HOSTNAME}.pem&quot;
tlsPrivateKeyFile: &quot;/var/lib/kubelet/${HOSTNAME}-key.pem&quot;
EOF

$ cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre><p id="381823bd-3888-4dd3-bad9-21c2ff46a8c2" class="">
</p><p id="cd74d90a-95f4-454e-afa6-73d7024036fd" class=""><strong>Configure the Kubernetes Proxy </strong></p><pre id="5cb2b1f1-dfc5-4e29-b4e0-309475e21550" class="code"><code>sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig

cat &lt;&lt;EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: &quot;/var/lib/kube-proxy/kubeconfig&quot;
mode: &quot;iptables&quot;
clusterCIDR: &quot;10.200.0.0/16&quot;
EOF

cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre><p id="27c5eccf-7605-4f29-adee-1230e6157ac0" class="">
</p><p id="fc19950c-cfcd-4c4f-8619-9d93f3d99caf" class=""><strong>Start Worker Services </strong></p><pre id="969237c5-2c39-42ba-a432-3d0f90ec7bfa" class="code"><code>$ sudo systemctl daemon-reload
$ sudo systemctl enable containerd kubelet kube-proxy
$ sudo systemctl start containerd kubelet kube-proxy</code></pre><p id="9e1af57e-f993-4872-b952-7930ffd02c47" class="">
</p><p id="902dc41f-f1b5-43e5-9498-c6fb2d047ab7" class=""><strong>Verification</strong></p><p id="a014ce56-b12f-4d44-ae9c-60643a4a111e" class="">Run from master node. </p><pre id="7f7c0a4a-4faa-4111-a8c0-3ac1e8495ca2" class="code"><code>$ kubectl get nodes --kubeconfig admin.kubeconfig

Expected output: 
NAME          STATUS   ROLES    AGE    VERSION
kubeworker1   Ready    &lt;none&gt;   4m5s   v1.15.3
kubeworker2   Ready    &lt;none&gt;   24s    v1.15.3</code></pre><p id="0f2b9bce-e462-4cac-8f3a-a6dd4fc3750c" class="">
</p><h1 id="e8a59bb4-79f2-4abb-8ded-458e1ee85032" class="">Remote Access kubectl</h1><p id="de235d1a-3ebb-451a-9173-44084aad6ed3" class="">Each kubeconfig requires a Kubernetes API Server to connect to. To support high availability the IP address assigned to the external load balancer fronting the Kubernetes API Servers will be used.</p><p id="95c4ace8-ecf6-479f-bbc6-b409eb8891c3" class="">
</p><p id="d61327d0-a9e0-449b-a7ca-87dbba96bfb1" class="">tbc </p><p id="c5b6f44f-fc4a-4c08-be25-77c2b9572906" class="">
</p><p id="a710de2e-bd6f-45e9-b129-9d0b69752faa" class="">
</p><p id="f7957d61-19d7-4a5f-a1d3-dec798650c23" class="">
</p><p id="7ad3707c-79d7-4500-97a7-c682192dd274" class="">
</p><p id="c9eb097d-6af7-4c90-9f5b-8c385cbf2f10" class="">
</p><p id="c2ccc0ff-464d-412d-bb26-19fd8dd40f8d" class="">
</p><p id="363f6747-b2a7-4e33-ad43-c6fcb9c1561b" class="">
</p><h2 id="f1acc562-2c44-46c6-bb61-3c92422f5fd9" class=""><mark class="highlight-blue">ALTERNATIVE</mark></h2><p id="b6720a2d-a07e-4c6f-bf36-ee1f1792af08" class="">
</p><p id="fb185bff-2298-4cf4-bd53-20ad405a7848" class=""><strong>Configure CNI Networking </strong></p><p id="ec7e0e64-45f9-4955-9b11-ac2e6cde4685" class="">Create the bridge network configuration file:</p><p id="d390a518-03df-4cfd-9039-f012e6df2b7a" class=""><mark class="highlight-red">POD_CIDR</mark></p><pre id="47f1e179-876c-4366-a4cd-9b62c6a7780f" class="code"><code>POD_CIDR=10.200.0.0/24
cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;bridge&quot;,
    &quot;type&quot;: &quot;bridge&quot;,
    &quot;bridge&quot;: &quot;cnio0&quot;,
    &quot;isGateway&quot;: true,
    &quot;ipMasq&quot;: true,
    &quot;ipam&quot;: {
        &quot;type&quot;: &quot;host-local&quot;,
        &quot;ranges&quot;: [
          [{&quot;subnet&quot;: &quot;${POD_CIDR}&quot;}]
        ],
        &quot;routes&quot;: [{&quot;dst&quot;: &quot;0.0.0.0/0&quot;}]
    }
}
EOF</code></pre><p id="9a6b9096-654b-4ea1-ad9b-5d36485f1efe" class="">Create the loopback network configuration file </p><pre id="4783a70a-346f-4bdf-a609-b14bcf2998d9" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;lo&quot;,
    &quot;type&quot;: &quot;loopback&quot;
}
EOF</code></pre><p id="284a5dd5-2f62-4646-9ab6-da002f014a0d" class="">
</p><p id="114b5d65-14ee-49e4-856a-463e115733bd" class="">
</p><p id="018b81ea-f3fe-473e-a4c3-aefc45085117" class=""><strong>Docker </strong></p><p id="133cc68c-a2ae-4250-865d-491c5d97ec58" class=""><a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">https://kubernetes.io/docs/setup/production-environment/container-runtimes/</a></p><p id="ea6b80fe-4643-4454-9b7b-769be8a94085" class="">Download </p><pre id="41205337-b368-4c6e-9684-fb56c4eee287" class="code"><code>$ sudo apt-get update
$ sudo apt-get install -y \
  apt-transport-https ca-certificates curl software-properties-common gnupg2

$ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

$ sudo add-apt-repository \
  &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) \
  stable&quot;

$ sudo apt-get update
$ sudo apt-get install -y \
  containerd.io=1.2.13-2 \
  docker-ce=5:19.03.11~3-0~ubuntu-$(lsb_release -cs) \
  docker-ce-cli=5:19.03.11~3-0~ubuntu-$(lsb_release -cs)

$ cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

$ mkdir -p /etc/systemd/system/docker.service.d

$ systemctl daemon-reload
$ systemctl restart docker
$ sudo systemctl enable docker</code></pre><p id="156bf7c5-0062-48b8-9bc3-0390cb870ea7" class="">
</p><p id="eca5bd4b-83b1-4a9e-94aa-9ca7be2fbbda" class=""><strong>Kubelet configuration</strong></p><p id="aefc6f5a-8362-4a30-b127-bac7b1ffaf55" class="">Move configs and certs </p><pre id="42dbff0d-1c00-46c3-81f5-cd212161d97f" class="code"><code>$ sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
$ sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
$ sudo mv ca.pem /var/lib/kubernetes/</code></pre><p id="356ca796-f586-4e6c-acb5-da680b105555" class="">Create the kubelet-config.yaml configuration file:</p><p id="d954f9aa-ed7a-48eb-b3e2-174861c32221" class=""><mark class="highlight-red">POD CIDR defined, clusterDNS </mark></p><p id="96708d52-a35b-4048-be80-c851d4630031" class="">10.200.${i}.0/24</p><pre id="e9b60ef0-e593-4900-9858-262011310a88" class="code"><code>POD_CIDR=10.200.0.0/24 # for first worker 

cat &lt;&lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: &quot;/var/lib/kubernetes/ca.pem&quot;
authorization:
  mode: Webhook
clusterDomain: &quot;cluster.local&quot;
clusterDNS:
  - &quot;10.32.0.10&quot;
podCIDR: &quot;${POD_CIDR}&quot;
resolvConf: &quot;/run/systemd/resolve/resolv.conf&quot;
runtimeRequestTimeout: &quot;15m&quot;
tlsCertFile: &quot;/var/lib/kubelet/${HOSTNAME}.pem&quot;
tlsPrivateKeyFile: &quot;/var/lib/kubelet/${HOSTNAME}-key.pem&quot;
EOF</code></pre><p id="998ee37a-5e57-4b57-bc9b-0b584f82e927" class="">Create the kubelet.service systemd unit file:</p><pre id="8a606f29-b662-493d-bafa-f9f39213f48a" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service.d
Requires=docker.service.d

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/docker/metrics.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre><p id="0f3985ad-c991-44e9-83d9-63d5d35a427f" class="">
</p><p id="b97c99cf-8243-414f-9c99-023f24addff6" class=""><strong>Kubernetes Proxy Configuration</strong></p><pre id="948bcf8a-9474-4191-8605-feb470ec0696" class="code"><code>sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig</code></pre><p id="b25926ac-efb5-4f3e-b79b-8918079398d8" class="">Create the kube-proxy-config.yaml configuration file:</p><pre id="7b884daa-c004-427b-8600-b693a34c5522" class="code"><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</code></pre><p id="f5201162-787a-4f92-be85-310101625203" class="">
</p><p id="3c2e3266-1775-428d-87c9-6318da788a42" class=""><strong>Start Worker Services </strong></p><pre id="bda1f11e-1966-472a-8e0f-4d9f6a8ee549" class="code"><code>$ sudo systemctl daemon-reload
$ sudo systemctl enable containerd kubelet kube-proxy
$ sudo systemctl start containerd kubelet kube-proxy</code></pre><p id="686221d6-80d7-4989-969b-f33625a37153" class="">
</p><p id="7114b427-66c5-4735-8c7e-d9d3020deef1" class=""><strong>Kubernetes Container Runtime Interface (CRI) </strong></p><p id="ee68243b-bf22-4e0b-bc29-1e115d5e57fb" class="">A kubelet runs on each worker node of a Kubernetes enabled cluster. Acting as the primary node agent, it is the primary of the pods and the node application programming interfaces (APIs) that drive the container execution layer. </p><p id="861637a2-90ec-4408-92ce-e6a5946da71c" class="">Pods can host multiple application containers and storage volumes. Pods are the fundamental execution primitive of Kubernetes. After kubelet gets the configuration of a pod through its pod spec, it ensures that the specified containers for the pod are up and running. </p><p id="8ed79169-fbe9-484a-a7ec-78e463223e40" class="">To create a pod, kubelet needs a container runtime environment that is Open Container Initiative (OCI) compliant, ensuring compatibility with the Kubernetes Container Runtime Interface (CRI). OCI includes a set of specifications that container runtime engines must implement and a seed container runtime engine called runc. </p><p id="5f93209c-cf02-4237-a4fd-8f58fb903c06" class=""><a href="https://developer.ibm.com/technologies/containers/blogs/kube-cri-overview/">https://developer.ibm.com/technologies/containers/blogs/kube-cri-overview/</a></p><p id="3094ddcf-095a-41cc-96a6-f1cefb2437fa" class="">
</p><hr id="0f4fb97f-83a9-45e0-be67-57761eb370d9"/><hr id="697db666-ad0b-40f5-8be1-36df502c05eb"/><hr id="69082230-2a60-4d4b-98c5-fe639ee89b74"/><p id="21ea47ba-702c-4797-9cd5-dc192415b813" class="">
</p><hr id="5d0bb6f7-7ebe-4b44-921a-d856f45c48d7"/><hr id="423804fd-2dca-49da-a460-0c4be1383595"/><hr id="4a3fbb04-299e-400f-9093-e355b0e1a810"/><hr id="fddd3c98-aeeb-41dc-8298-d02efacef0cc"/><p id="4a7c2668-3949-4ae7-8067-cdf62c843ffd" class="">
</p><p id="7b148636-701d-4bcc-986b-f9d9fe3e8307" class="">
</p><p id="c4d81c57-1441-4b3c-9d8d-9705779862a5" class="">
</p><h1 id="25b87a32-c613-4873-834a-e1e8acf4d64f" class="">Networking</h1><p id="bd60dc71-3660-430a-8e23-6bcf573f0986" class=""><a href="https://neuvector.com/network-security/advanced-kubernetes-networking/">https://neuvector.com/network-security/advanced-kubernetes-networking/</a></p><p id="371405dc-7bb0-4a3a-b11b-9156e1c57711" class=""><a href="https://blog.octo.com/en/author/sebastian-caceres-sca/page/2/">https://blog.octo.com/en/author/sebastian-caceres-sca/page/2/</a></p><p id="14d188a7-e59a-482d-b7f4-b1976a374901" class=""><a href="https://blog.octo.com/en/how-does-it-work-kubernetes-episode-5-master-and-worker-at-last/">https://blog.octo.com/en/how-does-it-work-kubernetes-episode-5-master-and-worker-at-last/</a></p><p id="f9a03487-6f3c-4161-bdc6-285be5ef1dad" class="">
</p><p id="2cb0be39-4971-4e0a-b105-774d47b8b264" class="">Flannel </p><p id="eeb36f7c-ce21-4240-8966-b2a664d90112" class=""><a href="https://github.com/coreos/flannel-cni/tree/v0.3.0#readme">https://github.com/coreos/flannel-cni/tree/v0.3.0#readme</a></p><p id="0c43cdf5-cf22-4b2b-879b-7147b5cbb804" class=""><a href="https://github.com/coreos/flannel/blob/master/Documentation/running.md">https://github.com/coreos/flannel/blob/master/Documentation/running.md</a></p><p id="0a2105e1-6182-4474-95cb-6a98fba25b00" class=""><a href="https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c">https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c</a></p><p id="ea3f9f51-1c5d-45e9-8b2e-3cc4ee8e523b" class=""><a href="https://github.com/coreos/flannel-cni/tree/v0.3.0#readme">https://github.com/coreos/flannel-cni/tree/v0.3.0#readme</a></p><p id="8daff827-feba-49b8-86f0-96b68ad7a623" class=""><a href="https://neuvector.com/network-security/advanced-kubernetes-networking/">https://neuvector.com/network-security/advanced-kubernetes-networking/</a></p><p id="c059775e-c1ba-44d5-a1b0-8bfa2685d169" class="">
</p><p id="0a318ab7-1c9b-41e3-93cc-45000861e090" class="">The official documentation
<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a>
says kubernetes networking model requires:</p><ul id="46f63a3e-6523-4b6e-b870-a3bee06aacaa" class="bulleted-list"><li>all containers can communicate with all other containers without NAT</li></ul><ul id="71ea1d37-e9cd-494b-b541-33c3292dca5c" class="bulleted-list"><li>all nodes can communicate with all containers (and vice-versa) without NAT</li></ul><ul id="6fde42cd-cd65-41c8-b48c-a9ee4c8c6cd5" class="bulleted-list"><li>the IP that a container see iteself as is the same IP that others see it as
Since containers share a Pods networks, the above requirements apply to pods.
In short, all Pods should be able to freely communicate with any other Pods in the cluster, even if they are in different Hosts, and they recognize each other with their own IP address. The Host should be able to communicate with any Pod with it&#x27;s own IP address, without any address translation.</li></ul><p id="bcf9914c-308f-4c80-924c-b982697e7454" class="">Kubernetes does not provide any default network implementation, rather it only defines the model and leaves implementation to other tools, e.g. flannel.</p><p id="e7bc25d5-3a0a-45b3-9ce6-f5478310b452" class=""><a href="https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c">https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c</a><a href="https://createnetech.tistory.com/30">https://createnetech.tistory.com/30</a><a href="https://github.com/coreos/flannel/blob/master/Documentation/running.md">https://github.com/coreos/flannel/blob/master/Documentation/running.md</a></p><p id="0e194efc-1524-4924-bc54-adeb2b187f62" class="">Flannel - The Overlay Network
To achieve kubernetes&#x27; network requirements flannel:</p><ul id="b58b10eb-37a8-4a1a-97e2-e82282e03d7e" class="bulleted-list"><li>creates another flat network whcih runs above the hostr network (overlay)</li></ul><ul id="6aa1726c-023f-464e-afcc-d074cec4bde5" class="bulleted-list"><li>Assigns all containers (Pod) an IP address in this overlay network</li></ul><ul id="b30a2d98-a6f8-43b6-a2d1-af35c1c73de3" class="bulleted-list"><li>Pods communicate with each other by calling the other&#x27;s ip address directly</li></ul><h1 id="8e05938f-8a31-4cfd-b573-c21e861300e1" class="">Pods and Containers</h1><p id="996fe30d-869a-4b9b-8837-623e37fad572" class=""><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</a></p><p id="307a48eb-d25e-4ace-b4d3-b3342e6c61b9" class="">
</p><p id="a9fa5131-ae9f-4c57-a923-f5538c34fade" class="">
</p><h1 id="952e6451-153c-4ca6-9ca0-490d22e55385" class="">Demo </h1><p id="c269f6d9-b663-45d2-a000-364c6f94d90e" class=""><a href="https://www.ulam.io/blog/kubernetes-scratch/">https://www.ulam.io/blog/kubernetes-scratch/</a></p><p id="c35ffc0f-7edb-4b0a-acea-e4a6c39dee5a" class="">
</p><p id="c8211fd2-ea1b-4939-a720-ef661ec21b6e" class="">ETCD </p><pre id="4eab8cdf-fe1d-4e03-84f1-abec76f476f3" class="code"><code>export GOROOT=/usr/lib/go
export GOPATH=$HOME/go
export PATH=$PATH:$GOROOT/bin:$GOPATH/bin</code></pre><p id="1a74b4a2-a6fa-41c8-b33b-5325bb96585c" class=""><a href="https://unofficial-kubernetes.readthedocs.io/en/latest/getting-started-guides/fedora/flannel_multi_node_cluster/#prerequisites">https://unofficial-kubernetes.readthedocs.io/en/latest/getting-started-guides/fedora/flannel_multi_node_cluster/#prerequisites</a></p><pre id="42354d97-2765-46e2-a6e2-9b2f7f1b8b34" class="code"><code># Set 
sudo ETCDCTL_API=3 etcdctl put /coreos.com/network/config &lt; flannel-config.json --endpoints=https://127.0.0.1:2379  --cacert=/etc/etcd/ca.pem  --cert=/etc/etcd/kubernetes.pem  --key=/etc/etcd/kubernetes-key.pem
# Check
sudo ETCDCTL_API=3 etcdctl get /coreos.com/network/config --endpoints=https://127.0.0.1:2379  --cacert=/etc/etcd/ca.pem  --cert=/etc/etcd/kubernetes.pem  --key=/etc/etcd/kubernetes-key.pem
</code></pre><p id="0e5675af-f4b2-4d99-8366-25e508f02e7d" class="">GO </p><pre id="a914f8e4-c029-48e5-b905-2e015c08748f" class="code"><code>$sudo apt install golang -y</code></pre><p id="02904e35-8d43-4276-8555-7452c8787fc8" class="">Just add the following lines to ~/.bashrc (Of your user)</p><pre id="ee840cab-aa22-4af3-acd8-6bb666d63ee8" class="code"><code>export GOROOT=/usr/lib/go
export GOPATH=$HOME/go
export PATH=$PATH:$GOROOT/bin:$GOPATH/bin</code></pre><p id="577b05b5-94bc-443f-b43e-5fb9d38b6f8d" class="">GO - Flannel </p><pre id="efe0eb08-a965-4355-b1e3-d6172559fac7" class="code"><code>cd ~
export GOPATH=$(pwd)
mkdir -p src/github.com/coreos
cd src/github.com/coreos
git clone https://github.com/coreos/flannel.git
cd flannel
make dist/flanneld</code></pre><p id="acbc8540-8667-4cbf-ba66-5aaba411a4e4" class=""><a href="https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step-Part4/">https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step-Part4/</a></p><p id="30005a51-d406-4d12-a32c-25a4fc90aa86" class="">
</p><p id="182eee03-8781-4517-978a-f7ab74205ad1" class="">
</p></div></article></body></html>